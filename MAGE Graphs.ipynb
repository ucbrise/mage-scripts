{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse output of end-to-end benchmarks\n",
    "class MageMeasurement(object):\n",
    "    def __init__(self, f):\n",
    "        lines = []\n",
    "        for line in f:\n",
    "            tokens = line.split()\n",
    "            if len(tokens) >= 2 and tokens[1] == \"(ns)\":\n",
    "                stats = Stats(line)\n",
    "                if stats.name not in self.stats:\n",
    "                    self.stats[stats.name] = []\n",
    "                self.stats[stats.name].append(stats.total)\n",
    "            elif len(tokens) == 3 and tokens[0] == \"Timer:\" and tokens[2] == \"ns\":\n",
    "                self.time_for_computation_ns = int(tokens[1])\n",
    "                self.time_for_computation_ms = self.time_for_computation_ns / 1000000.0\n",
    "            lines.append(line)\n",
    "        \n",
    "        tokens = lines[-1].split()\n",
    "        if len(tokens) == 2 and tokens[1] == \"ms\":\n",
    "            self.total_time_ms = int(tokens[0])\n",
    "        else:\n",
    "            assert(False)\n",
    "            self.total_time_ms = self.time_for_computation_ns / 1000000.0 # Hack to produce graphs with some missing data\n",
    "\n",
    "class EMPMeasurement(object):\n",
    "    def __init__(self, f):\n",
    "        lines = []\n",
    "        for line in f:\n",
    "            lines.append(line)\n",
    "        assert(len(lines) == 4)\n",
    "        assert(lines[0] == \"connected\\n\")\n",
    "        assert(lines[2] == \"PASS\\n\")\n",
    "        self.time_for_computation_ms = int(lines[1].split()[1])\n",
    "        self.total_time_ms = int(lines[3].split()[1])\n",
    "        \n",
    "class PlanningMeasurement(object):\n",
    "    def __init__(self, f):\n",
    "        lines = []\n",
    "        for line in f:\n",
    "            lines.append(line)\n",
    "        assert(len(lines) == 6)\n",
    "        phase_times = lines[5].split()\n",
    "        self.placement_ms = int(phase_times[3])\n",
    "        self.replacement_ms = int(phase_times[4])\n",
    "        self.scheduling_ms = int(phase_times[5])\n",
    "        self.total_ms = self.placement_ms + self.replacement_ms + self.scheduling_ms\n",
    "        \n",
    "class SEALMeasurement(object):\n",
    "    def __init__(self, f):\n",
    "        lines = []\n",
    "        for line in f:\n",
    "            lines.append(line)\n",
    "        assert(len(lines) == 1)\n",
    "        self.total_time_ms = int(lines[0].split()[0])\n",
    "\n",
    "class Stats(object):\n",
    "    def __init__(self, line):\n",
    "        tokens = line.strip().split()\n",
    "        self.name = tokens[0]\n",
    "        self.unit = tokens[1][1:-2]\n",
    "        assert(tokens[3] == \"min\")\n",
    "        assert(tokens[6] == \"avg\")\n",
    "        assert(tokens[9] == \"max\")\n",
    "        assert(tokens[12] == \"count\")\n",
    "        assert(tokens[15] == \"sum\")\n",
    "        self.total = int(tokens[17])\n",
    "        \n",
    "def parse_planning_log(f):\n",
    "    last_line = None\n",
    "    for line in f:\n",
    "        last_line = line\n",
    "    tokens = last_line.split()\n",
    "    return tuple(int(x) for x in tokens[3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_filename(prefix, program, scenario, tag):\n",
    "    if isinstance(tag, int):\n",
    "        tag = \"t{0}\".format(tag)\n",
    "    return \"{0}_{1}_{2}_{3}.log\".format(prefix, program, scenario, tag)\n",
    "\n",
    "def parse_emp_measurement_file(filename):\n",
    "    with open(filename) as f:\n",
    "        return EMPMeasurement(f)\n",
    "\n",
    "def parse_mage_measurement_file(filename):\n",
    "    with open(filename) as f:\n",
    "        return MageMeasurement(f)\n",
    "    \n",
    "def parse_seal_measurement_file(filename):\n",
    "    with open(filename) as f:\n",
    "        return SEALMeasurement(f)\n",
    "    \n",
    "def parse_planning_measurement_file(filename):\n",
    "    with open(filename) as f:\n",
    "        return PlanningMeasurement(f)\n",
    "    \n",
    "def parse_planning_log_file(filename):\n",
    "    with open(filename) as f:\n",
    "        return parse_planning_log(f)\n",
    "    \n",
    "def parse_log_directory(directory):\n",
    "    logs = {}\n",
    "    machine_directories = !ls {directory}\n",
    "    for mdir in machine_directories:\n",
    "        try:\n",
    "            machine_id = int(mdir)\n",
    "        except ValueError:\n",
    "            print(\"Skipping directory {0}\", os.path.join(\"directory\", mdir))\n",
    "            continue\n",
    "            \n",
    "        machine_logs = logs.setdefault(machine_id, {})\n",
    "            \n",
    "        log_files = !ls {os.path.join(directory, mdir)}\n",
    "        for log_file in log_files:\n",
    "            if log_file.endswith(\".log\") or log_file.endswith(\".planning\"):\n",
    "                log_path = os.path.join(directory, mdir, log_file)\n",
    "                \n",
    "                parts = log_file.split(\".\")\n",
    "                extension = parts[-1]\n",
    "                name = \".\".join(parts[:-1])\n",
    "                \n",
    "                ext_logs = machine_logs.setdefault(extension, {})\n",
    "                \n",
    "                tokens = name.split(\"_\")\n",
    "                if len(tokens) < 6:\n",
    "                    print(\"Skipping file {0}\".format(log_path))\n",
    "                    continue\n",
    "                experiment = \"_\".join(tokens[:2])\n",
    "                scenario = tokens[-2]\n",
    "                try:\n",
    "                    size = int(tokens[-3])\n",
    "                except ValueError:\n",
    "                    print(\"Skipping file {0}\".format(log_path))\n",
    "                    continue\n",
    "                problem = \"_\".join(tokens[2:-3])\n",
    "                \n",
    "                tag = tokens[-1]\n",
    "                \n",
    "                experiments = ext_logs.setdefault(experiment, {}).setdefault(problem, {}).setdefault(size, {}).setdefault(scenario, {})\n",
    "                if tag in experiments:\n",
    "                    print(\"Skipping {0} (duplicate for {1})\".format(log_path, (experiment, problem, size, scenario, tag)))\n",
    "                    continue\n",
    "                    \n",
    "                try:\n",
    "                    if scenario in (\"os\", \"unbounded\", \"mage\"):\n",
    "                        if extension == \"log\":\n",
    "                            parsed = parse_mage_measurement_file(log_path)\n",
    "                        elif extension == \"planning\":\n",
    "                            parsed = parse_planning_measurement_file(log_path)\n",
    "                    elif scenario == \"emp\":\n",
    "                        parsed = parse_emp_measurement_file(log_path)\n",
    "                    elif scenario == \"seal\":\n",
    "                        parsed = parse_seal_measurement_file(log_path)\n",
    "                    else:\n",
    "                        print(\"Skipping {0} (unknown scenario {1})\".format(log_path, scenario))\n",
    "                        continue\n",
    "                    experiments[tag] = parsed\n",
    "                except AssertionError as ae:\n",
    "                    print(\"Skipping {0} (assertion failure: {1})\".format(log_path, str(ae)))\n",
    "    return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster2_logs = parse_log_directory(\"logs-2\")\n",
    "cluster8_logs = parse_log_directory(\"logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline Experiments (Figures 6 and 7)\n",
    "================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_unbounded_style = {\"label\": \"Unbounded\", \"color\": \"blue\", \"marker\": \"s\", \"ls\": \"-\", \"linewidth\": 3}       \n",
    "baseline_mage_style = {\"label\": \"MAGE 1 GiB\", \"color\": \"orange\", \"marker\": \"^\", \"ls\": \"--\"}\n",
    "baseline_os_style = {\"label\": \"OS 1 GiB\", \"color\": \"green\", \"marker\": \"o\", \"ls\": \"-\"}\n",
    "baseline_emp_style = {\"label\": \"EMP 1 GiB\", \"color\": \"red\", \"marker\": \"v\", \"ls\": \"-.\"}\n",
    "baseline_seal_style = {\"label\": \"SEAL 1 GiB\", \"color\": \"red\", \"marker\": \"v\", \"ls\": \"-.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_halfgates_baseline_graph(sizes):\n",
    "    plt.figure(figsize = (3, 2))\n",
    "    \n",
    "    benchmark_data = cluster2_logs[0][\"log\"][\"halfgates_baseline\"][\"merge_sorted\"]\n",
    "    def get_data(scenario, sizes, tag):\n",
    "        result = []\n",
    "        for size in sizes:\n",
    "            time_s = benchmark_data[size][scenario][tag].time_for_computation_ms / 1000.0\n",
    "            result.append(time_s)\n",
    "        return result\n",
    "\n",
    "    plt.plot(sizes, get_data(\"unbounded\", sizes, \"t1\"), **baseline_unbounded_style)\n",
    "    plt.plot(sizes, get_data(\"os\", sizes, \"t1\"), **baseline_os_style)\n",
    "    plt.plot(sizes, get_data(\"mage\", sizes, \"t1\"), **baseline_mage_style)\n",
    "    plt.plot(sizes, get_data(\"emp\", sizes, \"t1\"), **baseline_emp_style)\n",
    "\n",
    "    plt.xlim(0, sizes[-1])\n",
    "    \n",
    "    plt.xlabel(\"Problem Size (Records Per Party)\")\n",
    "    plt.ylabel(\"Time (s)\")\n",
    "    \n",
    "def draw_ckks_baseline_graph(sizes):\n",
    "    plt.figure(figsize = (3, 2))\n",
    "    \n",
    "    benchmark_data = cluster2_logs[0][\"log\"][\"ckks_baseline\"][\"real_statistics\"]\n",
    "    def get_data(scenario, sizes, tag):\n",
    "        result = []\n",
    "        for size in sizes:\n",
    "            time_s = benchmark_data[size][scenario][tag].total_time_ms / 1000.0\n",
    "            result.append(time_s)\n",
    "        return result\n",
    "\n",
    "    plt.plot(sizes, get_data(\"unbounded\", sizes, \"t1\"), **baseline_unbounded_style)\n",
    "    plt.plot(sizes, get_data(\"os\", sizes, \"t1\"), **baseline_os_style)\n",
    "    plt.plot(sizes, get_data(\"mage\", sizes, \"t1\"), **baseline_mage_style)\n",
    "    plt.plot(sizes, get_data(\"seal\", sizes, \"t1\"), **baseline_seal_style)\n",
    "\n",
    "    plt.xlim(0, sizes[-1])\n",
    "    \n",
    "    plt.xlabel(\"Problem Size (Number of Elements)\")\n",
    "    plt.ylabel(\"Time (s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_halfgates_baseline_graph(tuple(2 ** i for i in range(13, 20)))\n",
    "plt.ylim(0, 200)\n",
    "\n",
    "plt.legend(loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_halfgates_baseline_graph(tuple(2 ** i for i in range(13, 21)))\n",
    "plt.ylim(0, 1000)\n",
    "\n",
    "plt.xticks((0, 500000, 1000000))\n",
    "plt.ticklabel_format(style = \"plain\")\n",
    "\n",
    "plt.legend(loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_ckks_baseline_graph(tuple(2 ** i for i in range(8, 13)))\n",
    "plt.ylim(0, 10)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_ckks_baseline_graph(tuple(2 ** i for i in range(8, 15)))\n",
    "plt.ylim(0, 250)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single-Node Experiments\n",
    "====================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barwidth = 0.2\n",
    "errbarsize = 3\n",
    "styles = [{\"label\": \"Unbounded\", \\\n",
    "            \"width\": barwidth, \"capsize\": errbarsize, \"color\": \"blue\", \"hatch\": \"\", \"edgecolor\": \"black\"},          \n",
    "          {\"label\": \"MAGE 1 GiB\", \\\n",
    "           \"width\": barwidth, \"capsize\": errbarsize, \"color\": \"white\", \"hatch\": \"..\", \"edgecolor\": \"black\"},\n",
    "          {\"label\": \"OS 1 GiB\", \\\n",
    "           \"width\": barwidth, \"capsize\": errbarsize, \"color\": \"white\", \"hatch\": \"\\\\\\\\\", \"edgecolor\": \"black\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_workloads_plot(experiments, graph_data, experiment_display_names):\n",
    "    x = np.arange(0, len(experiments))\n",
    "    unbounded_err = [[(graph_data[e][\"unbounded\"][2] - graph_data[e][\"unbounded\"][1]) / graph_data[e][\"unbounded\"][2] for e in experiments], [(graph_data[e][\"unbounded\"][3] - graph_data[e][\"unbounded\"][2]) / graph_data[e][\"unbounded\"][2] for e in experiments]]\n",
    "    plt.bar(x - barwidth, [graph_data[e][\"unbounded\"][2] / graph_data[e][\"unbounded\"][2] for e in experiments], yerr = unbounded_err, **styles[0])\n",
    "    mage_err = [[(graph_data[e][\"mage\"][2] - graph_data[e][\"mage\"][1]) / graph_data[e][\"unbounded\"][2] for e in experiments], [(graph_data[e][\"mage\"][3] - graph_data[e][\"mage\"][2]) / graph_data[e][\"unbounded\"][2] for e in experiments]]\n",
    "    plt.bar(x, [graph_data[e][\"mage\"][2] / graph_data[e][\"unbounded\"][2] for e in experiments], yerr = mage_err, **styles[1])\n",
    "    os_err = [[(graph_data[e][\"os\"][2] - graph_data[e][\"os\"][1]) / graph_data[e][\"unbounded\"][2] for e in experiments], [(graph_data[e][\"os\"][3] - graph_data[e][\"os\"][2]) / graph_data[e][\"unbounded\"][2] for e in experiments]]\n",
    "    plt.bar(x + barwidth, [graph_data[e][\"os\"][2] / graph_data[e][\"unbounded\"][2] for e in experiments], yerr = os_err, **styles[2])\n",
    "    \n",
    "    plt.xticks(x, experiment_display_names)\n",
    "    plt.ylabel(\"Time (Normalized\\nby Unbounded)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hg_experiments = ((\"merge_sorted\", 1048576), (\"full_sort\", 1048576), (\"loop_join\", 2048), (\"matrix_vector_multiply\", 8192), (\"binary_fc_layer\", 16384))\n",
    "ckks_experiments = ((\"real_sum\", 65536), (\"real_statistics\", 16384), (\"real_matrix_vector_multiply\", 256), (\"real_naive_matrix_multiply\", 128), (\"real_tiled_matrix_multiply\", 128))\n",
    "\n",
    "experiments = hg_experiments + ckks_experiments\n",
    "experiment_display_names = (\"merge\\nn = 1048576\", \"sort\\nn = 1048576\", \"ljoin\\nn = 2048\", \"mvmul\\nn = 8192\", \"binfclayer\\nn = 16384\", \"rsum\\nn = 65536\", \"rstats\\nn = 16384\", \"rmvmul\\nn = 256\", \"n_rmatmul\\nn = 128\", \"t_rmatmul\\nn = 128\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 2))\n",
    "\n",
    "# TODO: Support multiple trials done in parallel across different pairs of machines\n",
    "benchmark_data = cluster2_logs[0][\"log\"][\"single_machine\"]\n",
    "graph_data = {}\n",
    "for i, e in enumerate(experiments):\n",
    "    graph_data[e] = {}\n",
    "    print(experiment_display_names[i].split(\"\\n\")[0], end = \"\")\n",
    "    for scenario in (\"unbounded\", \"os\", \"mage\"):\n",
    "        total_times = tuple(m.total_time_ms / 1000.0 for tag, m in benchmark_data[e[0]][e[1]][scenario].items())\n",
    "        assert len(total_times) > 0\n",
    "        graph_data[e][scenario] = np.percentile(total_times, (0, 25, 50, 75, 100))\n",
    "        print(\" &\", round(np.median(total_times), 1), end=\"\")\n",
    "    print(\" &\", round(graph_data[e][\"os\"][2] / graph_data[e][\"mage\"][2], 1), end=\"\")\n",
    "    print(\" &\", round((graph_data[e][\"mage\"][2] / graph_data[e][\"unbounded\"][2] - 1) * 100, 0), end=\"\\\\%\")\n",
    "    print()\n",
    "\n",
    "draw_workloads_plot(experiments, graph_data, experiment_display_names)\n",
    "\n",
    "# Uncomment this line for the scale to match the one in the figure in the paper.\n",
    "# plt.ylim(0, 11)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Support multiple trials done in parallel across different pairs of machines\n",
    "planning_data = cluster2_logs[0][\"planning\"][\"single_machine\"]\n",
    "for i, e in enumerate(experiments):\n",
    "    graph_data[e] = {}\n",
    "    print(experiment_display_names[i].split(\"\\n\")[0], end = \"\")\n",
    "    planning_times = tuple(m.total_ms / 1000.0 for tag, m in planning_data[e[0]][e[1]][\"mage\"].items())\n",
    "    assert len(planning_times) > 0\n",
    "    graph_data[e][scenario] = np.percentile(planning_times, (0, 25, 50, 75, 100))\n",
    "    median = np.median(planning_times)\n",
    "    if median < 0.1:\n",
    "        rounded_median = round(median, 3)\n",
    "    elif median < 1:\n",
    "        rounded_median = round(median, 2)\n",
    "    else:\n",
    "        rounded_median = round(median, 1)\n",
    "    print(\" &\", rounded_median, end=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p = 4 Parallelism Experiments\n",
    "========================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hg_experiments_4 = ((\"merge_sorted\", 4194304), (\"full_sort\", 4194304), (\"loop_join\", 4096), (\"matrix_vector_multiply\", 16384), (\"binary_fc_layer\", 32768))\n",
    "ckks_experiments_4 = ((\"real_sum\", 262144), (\"real_statistics\", 65536), (\"real_matrix_vector_multiply\", 512), (\"real_naive_matrix_multiply\", 256), (\"real_tiled_matrix_multiply\", 256))\n",
    "\n",
    "experiments_4 = hg_experiments_4 + ckks_experiments_4\n",
    "experiment_4_display_names = (\"merge\\nn = 4194384\", \"sort\\nn = 4194384\", \"ljoin\\nn = 4096\", \"mvmul\\nn = 16384\", \"binfclayer\\nn = 32768\", \"rsum\\nn = 262144\", \"rstats\\nn = 32768\", \"rmvmul\\nn = 512\", \"n_rmatmul\\nn = 256\", \"t_rmatmul\\nn = 256\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 2))\n",
    "\n",
    "# TODO: Support multiple trials done in parallel across different pairs of machines\n",
    "benchmark_data = cluster2_logs[0][\"log\"][\"single_machine\"]\n",
    "graph_data = {}\n",
    "for i, e in enumerate(experiments_4):\n",
    "    graph_data[e] = {}\n",
    "    print(experiment_4_display_names[i].split(\"\\n\")[0], end = \"\")\n",
    "    for scenario in (\"unbounded\", \"os\", \"mage\"):\n",
    "        instances_per_tag = {}\n",
    "        for machine in (0, 1, 2, 3):\n",
    "            machine_trials = cluster8_logs[machine][\"log\"][\"workers_4\"][e[0]][e[1]][scenario]\n",
    "            for tag, m in machine_trials.items():\n",
    "                instances_per_tag.setdefault(tag, []).append(m)\n",
    "                \n",
    "        total_times = []\n",
    "        for tag, machine_exps in instances_per_tag.items():\n",
    "            assert len(machine_exps) == 4\n",
    "            total_times.append(max(m.total_time_ms / 1000.0 for m in machine_exps))\n",
    "        assert len(total_times) > 0\n",
    "        graph_data[e][scenario] = np.percentile(total_times, (0, 25, 50, 75, 100))\n",
    "        print(\" &\", round(np.median(total_times), 1), end=\"\")\n",
    "    print(\" &\", round(graph_data[e][\"os\"][2] / graph_data[e][\"mage\"][2], 1), end=\"\")\n",
    "    print(\" &\", round((graph_data[e][\"mage\"][2] / graph_data[e][\"unbounded\"][2] - 1) * 100, 0), end=\"\\\\%\")\n",
    "    print()\n",
    "\n",
    "draw_workloads_plot(experiments_4, graph_data, experiment_4_display_names)\n",
    "\n",
    "# Uncomment this line for the scale to match the one in the figure in the paper.\n",
    "# plt.ylim(0, 11)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
